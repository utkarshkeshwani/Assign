{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assign 2.ipynb","version":"0.3.2","provenance":[{"file_id":"1gZYwZdkgXBJRr624SqWJ9f452BmKNkNT","timestamp":1539513085756},{"file_id":"1JURGwe4e5Z7928Zv2eiJVPCQNc702huM","timestamp":1521864568638}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"aNyZv-Ec52ot","colab_type":"text"},"source":["# **Import Libraries and modules**"]},{"cell_type":"code","metadata":{"id":"3m3w1Cw49Zkt","colab_type":"code","outputId":"59c43eca-e7b7-44ce-8aac-0068bca1a27a","executionInfo":{"status":"ok","timestamp":1564226865173,"user_tz":-330,"elapsed":10260,"user":{"displayName":"utkarsh keshwani","photoUrl":"https://lh3.googleusercontent.com/-pJISOj7ZDWg/AAAAAAAAAAI/AAAAAAAAKtc/OZ7PngT-q4k/s64/photo.jpg","userId":"10563007863886487317"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# https://keras.io/\n","!pip install -q keras\n","import keras"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"0ZL2lBiZvQyE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"93a66445-a2a8-46a1-c751-fa1e5991d09f","executionInfo":{"status":"ok","timestamp":1564226898458,"user_tz":-330,"elapsed":30881,"user":{"displayName":"utkarsh keshwani","photoUrl":"https://lh3.googleusercontent.com/-pJISOj7ZDWg/AAAAAAAAAAI/AAAAAAAAKtc/OZ7PngT-q4k/s64/photo.jpg","userId":"10563007863886487317"}}},"source":["import sys\n","import numpy\n","import string\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils\n","from keras.preprocessing.sequence import pad_sequences\n","\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WSMFcd0XvU0W","colab_type":"code","colab":{}},"source":["filename = \"/content/gdrive/My Drive/Colab Notebooks/EIP 3/wonderland.txt\"\n","raw_text = open(filename).read()\n","raw_text = raw_text.lower()\n","#raw_text = raw_text.translate(None, string.punctuation)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sbnurXrkn_Du","colab_type":"code","colab":{}},"source":["punctuations = '''!()-[]{};:'\"\\,<>/?@#$%^&*_~'''\n","\n","#my_str = \"Hello!!!, he said ---and went.\"\n","\n","# remove punctuation from the string\n","no_punct = \"\"\n","for char in raw_text:\n","   if char not in punctuations:\n","       no_punct = no_punct + char\n","\n","# display the unpunctuated string\n","raw_text = no_punct\n","raw_text1 = keras.preprocessing.text.hashing_trick(raw_text, 100, hash_function=None, filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t', lower=True, split='.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-lbnPM8Y6J4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"75b3b8ae-d3f6-4e29-9499-40fed220298a","executionInfo":{"status":"ok","timestamp":1564232490996,"user_tz":-330,"elapsed":960,"user":{"displayName":"utkarsh keshwani","photoUrl":"https://lh3.googleusercontent.com/-pJISOj7ZDWg/AAAAAAAAAAI/AAAAAAAAKtc/OZ7PngT-q4k/s64/photo.jpg","userId":"10563007863886487317"}}},"source":["print(raw_text1)"],"execution_count":47,"outputs":[{"output_type":"stream","text":["{0: 81, 1: 11, 2: 51, 3: 19, 4: 57, 5: 80, 6: 37, 7: 49, 8: 49, 9: 3, 10: 28, 11: 5, 12: 56, 13: 3, 14: 75, 15: 31, 16: 99, 17: 16, 18: 68, 19: 1, 20: 16, 21: 86, 22: 90, 23: 60, 24: 98, 25: 75, 26: 59, 27: 8, 28: 31, 29: 53, 30: 60, 31: 29, 32: 60, 33: 77, 34: 21, 35: 48, 36: 41, 37: 95, 38: 40, 39: 54, 40: 86, 41: 80, 42: 50, 43: 12, 44: 28, 45: 52, 46: 92, 47: 49, 48: 59, 49: 76, 50: 11, 51: 92, 52: 74, 53: 76, 54: 86, 55: 65, 56: 60, 57: 84, 58: 27, 59: 81, 60: 49, 61: 95, 62: 9, 63: 7, 64: 45, 65: 60, 66: 13, 67: 27, 68: 20, 69: 77, 70: 51, 71: 56, 72: 64, 73: 34, 74: 60, 75: 21, 76: 34, 77: 9, 78: 68, 79: 63, 80: 98, 81: 9, 82: 8, 83: 82, 84: 6, 85: 57, 86: 60, 87: 21, 88: 95, 89: 93, 90: 68, 91: 77, 92: 87, 93: 30, 94: 61, 95: 7, 96: 56, 97: 55, 98: 55, 99: 24, 100: 29, 101: 31, 102: 21, 103: 89, 104: 8, 105: 61, 106: 25, 107: 94, 108: 56, 109: 37, 110: 35, 111: 66, 112: 81, 113: 72, 114: 31, 115: 47, 116: 15, 117: 20, 118: 60, 119: 11, 120: 60, 121: 55, 122: 18, 123: 41, 124: 69, 125: 86, 126: 18, 127: 3, 128: 50, 129: 21, 130: 2, 131: 8, 132: 56, 133: 4, 134: 60, 135: 26, 136: 85, 137: 56, 138: 7, 139: 38, 140: 53, 141: 32, 142: 36, 143: 56, 144: 57, 145: 72, 146: 19, 147: 19, 148: 96, 149: 71, 150: 60, 151: 55, 152: 60, 153: 28, 154: 56, 155: 38, 156: 60, 157: 86, 158: 48, 159: 58, 160: 11, 161: 77, 162: 74, 163: 60, 164: 71, 165: 56, 166: 69, 167: 60, 168: 80, 169: 71, 170: 60, 171: 53, 172: 31, 173: 89, 174: 14, 175: 19, 176: 77, 177: 89, 178: 70, 179: 65, 180: 86, 181: 98, 182: 6, 183: 12, 184: 72, 185: 35, 186: 56, 187: 55, 188: 56, 189: 30, 190: 6, 191: 67, 192: 56, 193: 14, 194: 55, 195: 64, 196: 56, 197: 4, 198: 60, 199: 46, 200: 84, 201: 10, 202: 56, 203: 46, 204: 65, 205: 56, 206: 15, 207: 36, 208: 50, 209: 16, 210: 29, 211: 63, 212: 56, 213: 94, 214: 6, 215: 56, 216: 88, 217: 49, 218: 14, 219: 60, 220: 57, 221: 56, 222: 70, 223: 45, 224: 56, 225: 51, 226: 56, 227: 10, 228: 39, 229: 53, 230: 36, 231: 32, 232: 55, 233: 81, 234: 39, 235: 12, 236: 57, 237: 75, 238: 95, 239: 56, 240: 83, 241: 56, 242: 69, 243: 84, 244: 87, 245: 56, 246: 63, 247: 56, 248: 48, 249: 60, 250: 89, 251: 56, 252: 44, 253: 56, 254: 47, 255: 32, 256: 87, 257: 68, 258: 73, 259: 8, 260: 56, 261: 73, 262: 63, 263: 56, 264: 10, 265: 56, 266: 83, 267: 42, 268: 37, 269: 14, 270: 50, 271: 30, 272: 22, 273: 30, 274: 68, 275: 23, 276: 33, 277: 23, 278: 83, 279: 56, 280: 42, 281: 60, 282: 12, 283: 17, 284: 60, 285: 21, 286: 35, 287: 60, 288: 98, 289: 56, 290: 94, 291: 58, 292: 56, 293: 31, 294: 47, 295: 77, 296: 60, 297: 35, 298: 62, 299: 60, 300: 63, 301: 70, 302: 92, 303: 10, 304: 40, 305: 83, 306: 73, 307: 82, 308: 56, 309: 79, 310: 60, 311: 49, 312: 66, 313: 44, 314: 53, 315: 5, 316: 18, 317: 56, 318: 67, 319: 68, 320: 6, 321: 98, 322: 1, 323: 4, 324: 5, 325: 56, 326: 28, 327: 11, 328: 17, 329: 6, 330: 85, 331: 13, 332: 20, 333: 60, 334: 60, 335: 43, 336: 4, 337: 18, 338: 52, 339: 15, 340: 84, 341: 55, 342: 56, 343: 41, 344: 14, 345: 38, 346: 60, 347: 56, 348: 16, 349: 87, 350: 32, 351: 60, 352: 38, 353: 56, 354: 45, 355: 60, 356: 88, 357: 87, 358: 44, 359: 61, 360: 11, 361: 56, 362: 69, 363: 20, 364: 84, 365: 37, 366: 62, 367: 64, 368: 60, 369: 16, 370: 56, 371: 78, 372: 81, 373: 96, 374: 56, 375: 36, 376: 17, 377: 56, 378: 85, 379: 11, 380: 89, 381: 96, 382: 60, 383: 28, 384: 60, 385: 45, 386: 57, 387: 8, 388: 43, 389: 60, 390: 82, 391: 42, 392: 95, 393: 70, 394: 44, 395: 14, 396: 11, 397: 63, 398: 19, 399: 56, 400: 95, 401: 56, 402: 52, 403: 33, 404: 9, 405: 69, 406: 56, 407: 53, 408: 68, 409: 76, 410: 25, 411: 59, 412: 60, 413: 78, 414: 4, 415: 76, 416: 69, 417: 6, 418: 11, 419: 56, 420: 11, 421: 16, 422: 4, 423: 88, 424: 10, 425: 68, 426: 29, 427: 14, 428: 79, 429: 56, 430: 14, 431: 74, 432: 32, 433: 89, 434: 39, 435: 56, 436: 79, 437: 51, 438: 66, 439: 22, 440: 56, 441: 67, 442: 68, 443: 29, 444: 11, 445: 1, 446: 56, 447: 21, 448: 56, 449: 56, 450: 5, 451: 56, 452: 47, 453: 56, 454: 4, 455: 94, 456: 22, 457: 56, 458: 88, 459: 55, 460: 56, 461: 14, 462: 60, 463: 1, 464: 90, 465: 90, 466: 56, 467: 23, 468: 30, 469: 56, 470: 31, 471: 60, 472: 57, 473: 56, 474: 45, 475: 56, 476: 72, 477: 56, 478: 67, 479: 37, 480: 31, 481: 43, 482: 79, 483: 91, 484: 39, 485: 56, 486: 98, 487: 56, 488: 85, 489: 39, 490: 43, 491: 6, 492: 39, 493: 48, 494: 1, 495: 52, 496: 95, 497: 91, 498: 73, 499: 52, 500: 83, 501: 48, 502: 14, 503: 1, 504: 52, 505: 86, 506: 42, 507: 8, 508: 14, 509: 56, 510: 38, 511: 95, 512: 56, 513: 10, 514: 20, 515: 56, 516: 77, 517: 56, 518: 28, 519: 81, 520: 56, 521: 16, 522: 83, 523: 56, 524: 60, 525: 56, 526: 47, 527: 64, 528: 56, 529: 76, 530: 56, 531: 10, 532: 77, 533: 74, 534: 64, 535: 97, 536: 58, 537: 73, 538: 22, 539: 56, 540: 42, 541: 56, 542: 64, 543: 75, 544: 1, 545: 56, 546: 77, 547: 74, 548: 28, 549: 82, 550: 56, 551: 63, 552: 60, 553: 83, 554: 38, 555: 96, 556: 56, 557: 71, 558: 56, 559: 83, 560: 60, 561: 47, 562: 27, 563: 22, 564: 98, 565: 56, 566: 55, 567: 45, 568: 56, 569: 34, 570: 57, 571: 88, 572: 56, 573: 27, 574: 18, 575: 37, 576: 60, 577: 26, 578: 7, 579: 60, 580: 82, 581: 24, 582: 56, 583: 81, 584: 60, 585: 11, 586: 64, 587: 38, 588: 17, 589: 56, 590: 50, 591: 46, 592: 77, 593: 57, 594: 17, 595: 89, 596: 56, 597: 59, 598: 24, 599: 99, 600: 5, 601: 60, 602: 60, 603: 68, 604: 67, 605: 42, 606: 79, 607: 90, 608: 54, 609: 38, 610: 85, 611: 21, 612: 54, 613: 16, 614: 31, 615: 32, 616: 94, 617: 2, 618: 56, 619: 21, 620: 6, 621: 51, 622: 18, 623: 56, 624: 14, 625: 46, 626: 91, 627: 68, 628: 79, 629: 47, 630: 60, 631: 52, 632: 7, 633: 91, 634: 37, 635: 56, 636: 75, 637: 89, 638: 56, 639: 25, 640: 56, 641: 17, 642: 56, 643: 12, 644: 60, 645: 27, 646: 91, 647: 68, 648: 23, 649: 41, 650: 39, 651: 60, 652: 9, 653: 93, 654: 56, 655: 25, 656: 56, 657: 62, 658: 56, 659: 1, 660: 2, 661: 62, 662: 56, 663: 8, 664: 21, 665: 22, 666: 59, 667: 56, 668: 78, 669: 44, 670: 87, 671: 87, 672: 84, 673: 76, 674: 56, 675: 17, 676: 81, 677: 56, 678: 80, 679: 56, 680: 24, 681: 62, 682: 73, 683: 47, 684: 88, 685: 56, 686: 50, 687: 60, 688: 22, 689: 56, 690: 99, 691: 7, 692: 56, 693: 2, 694: 60, 695: 92, 696: 24, 697: 97, 698: 93, 699: 2, 700: 95, 701: 40, 702: 16, 703: 33, 704: 88, 705: 32, 706: 34, 707: 45, 708: 85, 709: 60, 710: 23, 711: 69, 712: 40, 713: 55, 714: 82, 715: 54, 716: 60, 717: 38, 718: 95, 719: 60, 720: 47, 721: 63, 722: 82, 723: 66, 724: 60, 725: 69, 726: 33, 727: 60, 728: 94, 729: 75, 730: 50, 731: 73, 732: 76, 733: 90, 734: 35, 735: 60, 736: 39, 737: 72, 738: 22, 739: 6, 740: 67, 741: 32, 742: 56, 743: 10, 744: 60, 745: 95, 746: 60, 747: 3, 748: 53, 749: 56, 750: 92, 751: 56, 752: 86, 753: 56, 754: 39, 755: 56, 756: 48, 757: 61, 758: 60, 759: 60, 760: 50, 761: 28, 762: 85, 763: 38, 764: 98, 765: 56, 766: 81, 767: 56, 768: 14, 769: 82, 770: 1, 771: 19, 772: 56, 773: 69, 774: 56, 775: 33, 776: 24, 777: 46, 778: 41, 779: 52, 780: 92, 781: 66, 782: 23, 783: 56, 784: 49, 785: 86, 786: 47, 787: 61, 788: 56, 789: 14, 790: 56, 791: 16, 792: 60, 793: 60, 794: 19, 795: 52, 796: 56, 797: 81, 798: 83, 799: 88, 800: 56, 801: 2, 802: 60, 803: 76, 804: 56, 805: 82, 806: 56, 807: 23, 808: 20, 809: 60, 810: 45, 811: 90, 812: 71, 813: 56, 814: 85, 815: 56, 816: 69, 817: 37, 818: 56, 819: 45, 820: 56, 821: 34, 822: 61, 823: 2, 824: 7, 825: 60, 826: 71, 827: 71, 828: 5, 829: 90, 830: 60, 831: 20, 832: 56, 833: 40, 834: 32, 835: 60, 836: 14, 837: 56, 838: 74, 839: 56, 840: 78, 841: 56, 842: 19, 843: 56, 844: 98, 845: 84, 846: 56, 847: 21, 848: 96, 849: 56, 850: 72, 851: 23, 852: 52, 853: 91, 854: 90, 855: 60, 856: 10, 857: 9, 858: 56, 859: 38, 860: 56, 861: 43, 862: 56, 863: 87, 864: 56, 865: 31, 866: 25, 867: 56, 868: 75, 869: 60, 870: 56, 871: 15, 872: 75, 873: 69, 874: 54, 875: 48, 876: 81, 877: 60, 878: 69, 879: 34, 880: 48, 881: 56, 882: 4, 883: 60, 884: 71, 885: 56, 886: 8, 887: 56, 888: 31, 889: 15, 890: 7, 891: 18, 892: 57, 893: 68, 894: 4, 895: 60, 896: 82, 897: 81, 898: 60, 899: 36, 900: 30, 901: 32, 902: 56, 903: 9, 904: 40, 905: 48, 906: 68, 907: 82, 908: 56, 909: 64, 910: 64, 911: 1, 912: 56, 913: 6, 914: 56, 915: 9, 916: 11, 917: 39, 918: 56, 919: 68, 920: 45, 921: 60, 922: 61, 923: 88, 924: 56, 925: 71, 926: 84, 927: 72, 928: 56, 929: 83, 930: 56, 931: 6, 932: 60, 933: 15, 934: 46, 935: 39, 936: 56, 937: 83, 938: 60, 939: 62, 940: 18, 941: 13, 942: 68, 943: 56, 944: 60, 945: 4, 946: 56, 947: 21, 948: 31, 949: 56, 950: 85, 951: 41, 952: 60, 953: 7, 954: 68, 955: 41, 956: 4, 957: 87, 958: 56, 959: 83, 960: 5, 961: 73, 962: 32, 963: 56, 964: 49, 965: 42, 966: 76, 967: 56, 968: 51, 969: 28, 970: 40, 971: 60, 972: 93, 973: 56, 974: 8, 975: 70, 976: 56, 977: 70, 978: 56, 979: 86, 980: 97, 981: 56, 982: 61, 983: 56, 984: 67, 985: 60, 986: 73, 987: 47, 988: 56, 989: 25, 990: 56, 991: 91, 992: 60, 993: 24, 994: 40, 995: 60, 996: 77, 997: 28, 998: 56, 999: 7, 1000: 56, 1001: 49, 1002: 56, 1003: 96, 1004: 28, 1005: 56, 1006: 51, 1007: 52, 1008: 45, 1009: 56, 1010: 39, 1011: 56, 1012: 74, 1013: 18, 1014: 56, 1015: 42, 1016: 47, 1017: 60, 1018: 56, 1019: 13, 1020: 56, 1021: 33, 1022: 39, 1023: 56, 1024: 14, 1025: 53, 1026: 56, 1027: 3, 1028: 56, 1029: 11, 1030: 67, 1031: 60, 1032: 51, 1033: 16, 1034: 56, 1035: 93, 1036: 98, 1037: 10, 1038: 56, 1039: 88, 1040: 44, 1041: 56, 1042: 56, 1043: 60, 1044: 97, 1045: 60, 1046: 35, 1047: 39, 1048: 56, 1049: 11, 1050: 56, 1051: 9, 1052: 97, 1053: 55, 1054: 76, 1055: 30, 1056: 50, 1057: 13, 1058: 91, 1059: 68, 1060: 81, 1061: 56, 1062: 73, 1063: 12, 1064: 8, 1065: 56, 1066: 98, 1067: 29, 1068: 64, 1069: 56, 1070: 23, 1071: 55, 1072: 58, 1073: 92, 1074: 26, 1075: 29, 1076: 24, 1077: 69, 1078: 56, 1079: 55, 1080: 60, 1081: 19, 1082: 60, 1083: 89, 1084: 68, 1085: 76, 1086: 73, 1087: 36, 1088: 64, 1089: 60, 1090: 82, 1091: 91, 1092: 88, 1093: 11, 1094: 88, 1095: 66, 1096: 65, 1097: 6, 1098: 4, 1099: 43, 1100: 60, 1101: 68, 1102: 16, 1103: 56, 1104: 60, 1105: 56, 1106: 88, 1107: 25, 1108: 16, 1109: 93, 1110: 56, 1111: 34, 1112: 41, 1113: 92, 1114: 73, 1115: 58, 1116: 44, 1117: 56, 1118: 52, 1119: 96, 1120: 34, 1121: 49, 1122: 25, 1123: 81, 1124: 82, 1125: 26, 1126: 11, 1127: 56, 1128: 65, 1129: 27, 1130: 55, 1131: 92, 1132: 39, 1133: 52, 1134: 56, 1135: 18, 1136: 68, 1137: 35, 1138: 58, 1139: 79, 1140: 82, 1141: 6, 1142: 99, 1143: 95, 1144: 56, 1145: 1, 1146: 56, 1147: 55, 1148: 60, 1149: 93, 1150: 52, 1151: 26, 1152: 59, 1153: 75, 1154: 7, 1155: 60, 1156: 20, 1157: 56, 1158: 33, 1159: 39, 1160: 56, 1161: 5, 1162: 56, 1163: 20, 1164: 56, 1165: 74, 1166: 60, 1167: 42, 1168: 56, 1169: 70, 1170: 56, 1171: 88, 1172: 56, 1173: 58, 1174: 35, 1175: 56, 1176: 14, 1177: 72, 1178: 84, 1179: 2, 1180: 23, 1181: 56, 1182: 19, 1183: 56, 1184: 37, 1185: 56, 1186: 62, 1187: 75, 1188: 1, 1189: 64, 1190: 30, 1191: 60, 1192: 48, 1193: 60, 1194: 79, 1195: 36, 1196: 32, 1197: 11, 1198: 3, 1199: 16, 1200: 52, 1201: 68, 1202: 80, 1203: 48, 1204: 62, 1205: 56, 1206: 55, 1207: 82, 1208: 60, 1209: 35, 1210: 74, 1211: 50, 1212: 63, 1213: 56, 1214: 1, 1215: 74, 1216: 79, 1217: 56, 1218: 21, 1219: 93, 1220: 89, 1221: 43, 1222: 56, 1223: 68, 1224: 56, 1225: 93, 1226: 60, 1227: 56, 1228: 62, 1229: 25, 1230: 56, 1231: 3, 1232: 56, 1233: 20, 1234: 30, 1235: 56, 1236: 24, 1237: 60, 1238: 44, 1239: 56, 1240: 9, 1241: 36, 1242: 60, 1243: 37, 1244: 56, 1245: 31, 1246: 79, 1247: 4, 1248: 48, 1249: 99, 1250: 46, 1251: 60, 1252: 68, 1253: 16, 1254: 39, 1255: 92, 1256: 42, 1257: 57, 1258: 22, 1259: 97, 1260: 95, 1261: 24, 1262: 56, 1263: 85, 1264: 28, 1265: 1, 1266: 73, 1267: 4, 1268: 32, 1269: 62, 1270: 56, 1271: 53, 1272: 93, 1273: 11, 1274: 81, 1275: 47, 1276: 68, 1277: 22, 1278: 59, 1279: 56, 1280: 10, 1281: 56, 1282: 84, 1283: 60, 1284: 87, 1285: 11, 1286: 59, 1287: 75, 1288: 56, 1289: 41, 1290: 56, 1291: 83, 1292: 56, 1293: 33, 1294: 10, 1295: 92, 1296: 11, 1297: 56, 1298: 80, 1299: 56, 1300: 74, 1301: 68, 1302: 82, 1303: 15, 1304: 56, 1305: 86, 1306: 19, 1307: 80, 1308: 56, 1309: 8, 1310: 56, 1311: 62, 1312: 71, 1313: 8, 1314: 56, 1315: 32, 1316: 59, 1317: 13, 1318: 56, 1319: 27, 1320: 9, 1321: 55, 1322: 56, 1323: 88, 1324: 35, 1325: 56, 1326: 24, 1327: 13, 1328: 56, 1329: 79, 1330: 56, 1331: 34, 1332: 56, 1333: 18, 1334: 60, 1335: 15, 1336: 56, 1337: 8, 1338: 60, 1339: 20, 1340: 56, 1341: 49, 1342: 56, 1343: 18, 1344: 56, 1345: 43, 1346: 20, 1347: 68, 1348: 60, 1349: 56, 1350: 70, 1351: 56, 1352: 23, 1353: 35, 1354: 56, 1355: 88, 1356: 2, 1357: 81, 1358: 68, 1359: 9, 1360: 79, 1361: 98, 1362: 22, 1363: 60, 1364: 26, 1365: 56, 1366: 64, 1367: 56, 1368: 72, 1369: 56, 1370: 14, 1371: 34, 1372: 80, 1373: 60, 1374: 31, 1375: 39, 1376: 25, 1377: 68, 1378: 58, 1379: 53, 1380: 85, 1381: 10, 1382: 59, 1383: 60, 1384: 55, 1385: 56, 1386: 51, 1387: 56, 1388: 64, 1389: 60, 1390: 70, 1391: 77, 1392: 54, 1393: 79, 1394: 75, 1395: 60, 1396: 76, 1397: 62, 1398: 76, 1399: 56, 1400: 52, 1401: 46, 1402: 56, 1403: 70, 1404: 52, 1405: 89, 1406: 76, 1407: 51, 1408: 43, 1409: 56, 1410: 97, 1411: 36, 1412: 12, 1413: 94, 1414: 66, 1415: 77, 1416: 56, 1417: 42, 1418: 81, 1419: 46, 1420: 14, 1421: 56, 1422: 75, 1423: 13, 1424: 11, 1425: 88, 1426: 82, 1427: 34, 1428: 78, 1429: 56, 1430: 80, 1431: 56, 1432: 59, 1433: 66, 1434: 56, 1435: 30, 1436: 70, 1437: 18, 1438: 56, 1439: 76, 1440: 56, 1441: 71, 1442: 11, 1443: 56, 1444: 57, 1445: 56, 1446: 11, 1447: 56, 1448: 43, 1449: 60, 1450: 40, 1451: 56, 1452: 75, 1453: 39, 1454: 56, 1455: 54, 1456: 65, 1457: 60, 1458: 43, 1459: 56, 1460: 36, 1461: 56, 1462: 53, 1463: 43, 1464: 56, 1465: 69, 1466: 60, 1467: 1, 1468: 60, 1469: 48, 1470: 60, 1471: 65, 1472: 72, 1473: 88, 1474: 56, 1475: 63, 1476: 76, 1477: 60, 1478: 25, 1479: 42, 1480: 84, 1481: 56, 1482: 74, 1483: 56, 1484: 64, 1485: 85, 1486: 95, 1487: 56, 1488: 97, 1489: 84, 1490: 28, 1491: 56, 1492: 30, 1493: 13, 1494: 56, 1495: 77, 1496: 56, 1497: 32, 1498: 56, 1499: 25, 1500: 92, 1501: 56, 1502: 51, 1503: 56, 1504: 75, 1505: 3, 1506: 71, 1507: 60, 1508: 43, 1509: 8, 1510: 56, 1511: 12, 1512: 56, 1513: 26, 1514: 4, 1515: 75, 1516: 49, 1517: 60, 1518: 63, 1519: 31, 1520: 61, 1521: 65, 1522: 9, 1523: 76, 1524: 42, 1525: 33, 1526: 81, 1527: 60, 1528: 14, 1529: 30, 1530: 19, 1531: 85, 1532: 60, 1533: 77, 1534: 88, 1535: 56, 1536: 34, 1537: 63, 1538: 56, 1539: 23, 1540: 28, 1541: 28, 1542: 56, 1543: 38, 1544: 45, 1545: 81, 1546: 56, 1547: 10, 1548: 2, 1549: 56, 1550: 15, 1551: 56, 1552: 10, 1553: 56, 1554: 92, 1555: 56, 1556: 68, 1557: 56, 1558: 84, 1559: 56, 1560: 60, 1561: 56, 1562: 54, 1563: 60, 1564: 31, 1565: 79, 1566: 49, 1567: 60, 1568: 91, 1569: 94, 1570: 65, 1571: 35, 1572: 15, 1573: 78, 1574: 64, 1575: 41, 1576: 54, 1577: 31, 1578: 15, 1579: 18, 1580: 24, 1581: 38, 1582: 43, 1583: 73, 1584: 71, 1585: 84, 1586: 56, 1587: 45, 1588: 74, 1589: 56, 1590: 36, 1591: 60, 1592: 85, 1593: 56, 1594: 57, 1595: 5, 1596: 66, 1597: 36, 1598: 56, 1599: 14, 1600: 87, 1601: 29, 1602: 26, 1603: 94, 1604: 70, 1605: 56, 1606: 56, 1607: 61, 1608: 30, 1609: 59, 1610: 56, 1611: 69, 1612: 60, 1613: 26, 1614: 21, 1615: 60, 1616: 82, 1617: 31, 1618: 95, 1619: 6, 1620: 60, 1621: 31, 1622: 56, 1623: 38, 1624: 60, 1625: 41, 1626: 60, 1627: 10, 1628: 56, 1629: 43, 1630: 38, 1631: 99, 1632: 56, 1633: 37, 1634: 56, 1635: 12, 1636: 16, 1637: 56, 1638: 50, 1639: 56, 1640: 64, 1641: 92, 1642: 56, 1643: 27, 1644: 56, 1645: 75, 1646: 33, 1647: 2, 1648: 56, 1649: 53, 1650: 29, 1651: 18, 1652: 56, 1653: 49, 1654: 2, 1655: 56, 1656: 74, 1657: 56, 1658: 45, 1659: 89, 1660: 9, 1661: 14, 1662: 10, 1663: 21, 1664: 98, 1665: 56, 1666: 61, 1667: 56, 1668: 29, 1669: 60, 1670: 5, 1671: 69, 1672: 40, 1673: 56, 1674: 5, 1675: 56, 1676: 66, 1677: 29, 1678: 35, 1679: 47, 1680: 73, 1681: 53, 1682: 56, 1683: 21, 1684: 56, 1685: 60, 1686: 22, 1687: 34, 1688: 56, 1689: 52, 1690: 56, 1691: 70, 1692: 60, 1693: 44, 1694: 56, 1695: 51, 1696: 60, 1697: 92, 1698: 71, 1699: 19, 1700: 56, 1701: 47, 1702: 59, 1703: 45, 1704: 48, 1705: 8, 1706: 2, 1707: 40, 1708: 76, 1709: 56, 1710: 75, 1711: 60, 1712: 71, 1713: 58, 1714: 30, 1715: 53, 1716: 66, 1717: 39, 1718: 18, 1719: 56, 1720: 40, 1721: 56, 1722: 94, 1723: 4, 1724: 83, 1725: 24, 1726: 76, 1727: 1, 1728: 25, 1729: 60, 1730: 19, 1731: 55, 1732: 39, 1733: 56, 1734: 40, 1735: 49, 1736: 10, 1737: 67, 1738: 6, 1739: 10, 1740: 65, 1741: 23, 1742: 68, 1743: 34, 1744: 60, 1745: 74, 1746: 56, 1747: 85, 1748: 49, 1749: 56, 1750: 69, 1751: 18, 1752: 62, 1753: 19, 1754: 29, 1755: 60, 1756: 13, 1757: 52, 1758: 63, 1759: 97, 1760: 56, 1761: 60, 1762: 39, 1763: 74, 1764: 93, 1765: 56, 1766: 45, 1767: 60, 1768: 94, 1769: 91, 1770: 56, 1771: 8, 1772: 60, 1773: 68, 1774: 51, 1775: 56, 1776: 74, 1777: 60, 1778: 95, 1779: 60, 1780: 2, 1781: 56, 1782: 69, 1783: 56, 1784: 5, 1785: 56, 1786: 27, 1787: 56, 1788: 1, 1789: 56, 1790: 33, 1791: 56, 1792: 6, 1793: 56, 1794: 94, 1795: 26, 1796: 23, 1797: 61, 1798: 56, 1799: 75, 1800: 57, 1801: 89, 1802: 58, 1803: 56, 1804: 61, 1805: 58, 1806: 60, 1807: 22, 1808: 56, 1809: 84, 1810: 29, 1811: 56, 1812: 53, 1813: 56, 1814: 18, 1815: 11, 1816: 56, 1817: 41, 1818: 55, 1819: 35, 1820: 92, 1821: 35, 1822: 56, 1823: 28, 1824: 80, 1825: 56, 1826: 23, 1827: 47, 1828: 38, 1829: 56, 1830: 80, 1831: 56, 1832: 45, 1833: 68, 1834: 35, 1835: 24, 1836: 42, 1837: 10, 1838: 56, 1839: 84, 1840: 56, 1841: 50, 1842: 56, 1843: 76, 1844: 43, 1845: 56, 1846: 13, 1847: 56, 1848: 7, 1849: 46, 1850: 45, 1851: 56, 1852: 32, 1853: 56, 1854: 37, 1855: 98, 1856: 53, 1857: 60, 1858: 34, 1859: 56, 1860: 3, 1861: 15, 1862: 4, 1863: 29, 1864: 56, 1865: 33, 1866: 60, 1867: 20, 1868: 56, 1869: 38, 1870: 56, 1871: 84, 1872: 63, 1873: 56, 1874: 56, 1875: 28, 1876: 56, 1877: 82, 1878: 60, 1879: 75, 1880: 56, 1881: 4, 1882: 56, 1883: 71, 1884: 56, 1885: 58, 1886: 56, 1887: 90, 1888: 4, 1889: 36, 1890: 56, 1891: 48, 1892: 56, 1893: 47, 1894: 34, 1895: 70, 1896: 56, 1897: 13, 1898: 38, 1899: 73, 1900: 56, 1901: 83, 1902: 56, 1903: 59, 1904: 60, 1905: 52, 1906: 56, 1907: 82, 1908: 68, 1909: 67, 1910: 95, 1911: 57, 1912: 66, 1913: 23, 1914: 14, 1915: 99, 1916: 11, 1917: 56, 1918: 65, 1919: 56, 1920: 59, 1921: 7, 1922: 27, 1923: 80, 1924: 44, 1925: 39, 1926: 10, 1927: 56, 1928: 46, 1929: 56, 1930: 7, 1931: 56, 1932: 47, 1933: 56, 1934: 28, 1935: 56, 1936: 57, 1937: 62, 1938: 40, 1939: 56, 1940: 12, 1941: 74, 1942: 50, 1943: 93, 1944: 17, 1945: 32, 1946: 91, 1947: 30, 1948: 19, 1949: 36, 1950: 56, 1951: 77, 1952: 56, 1953: 98, 1954: 56, 1955: 62, 1956: 56, 1957: 55, 1958: 56, 1959: 11, 1960: 67, 1961: 56, 1962: 21, 1963: 56, 1964: 9, 1965: 30, 1966: 68, 1967: 48, 1968: 56, 1969: 76, 1970: 76, 1971: 56, 1972: 43, 1973: 56, 1974: 55, 1975: 39, 1976: 56, 1977: 54, 1978: 44, 1979: 56, 1980: 41, 1981: 56, 1982: 18, 1983: 17, 1984: 92, 1985: 5, 1986: 56, 1987: 94, 1988: 56, 1989: 25, 1990: 59, 1991: 41, 1992: 56, 1993: 93, 1994: 74, 1995: 56, 1996: 70, 1997: 72, 1998: 56, 1999: 16, 2000: 60, 2001: 27, 2002: 68, 2003: 56, 2004: 39, 2005: 56, 2006: 39, 2007: 60, 2008: 67, 2009: 78, 2010: 16, 2011: 60, 2012: 70, 2013: 56, 2014: 36, 2015: 59, 2016: 39, 2017: 19, 2018: 35, 2019: 60, 2020: 71, 2021: 21, 2022: 56, 2023: 54, 2024: 80, 2025: 79, 2026: 15, 2027: 54, 2028: 80, 2029: 87, 2030: 56, 2031: 19, 2032: 23, 2033: 5, 2034: 20, 2035: 95, 2036: 69, 2037: 60, 2038: 13, 2039: 83, 2040: 56, 2041: 80, 2042: 99, 2043: 79, 2044: 56, 2045: 60, 2046: 60, 2047: 3, 2048: 43, 2049: 56, 2050: 27, 2051: 63, 2052: 56, 2053: 73, 2054: 24, 2055: 56, 2056: 21, 2057: 19, 2058: 56, 2059: 59, 2060: 60, 2061: 40, 2062: 56, 2063: 71, 2064: 60, 2065: 27, 2066: 41, 2067: 56, 2068: 35, 2069: 56, 2070: 91, 2071: 56, 2072: 56, 2073: 72, 2074: 60, 2075: 32, 2076: 56, 2077: 67, 2078: 85, 2079: 16, 2080: 55, 2081: 33, 2082: 15, 2083: 7, 2084: 82, 2085: 75, 2086: 27}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f_Uxqe9AdbEJ","colab_type":"code","colab":{}},"source":["# create mapping of unique chars to integers\n","chars = sorted(list(set(raw_text)))\n","chars1 = sorted(list(set(raw_text1)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4w6OGQsdkBO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"bab133de-d8f9-4635-8b52-99def7d4cac5","executionInfo":{"status":"ok","timestamp":1564230900694,"user_tz":-330,"elapsed":1043,"user":{"displayName":"utkarsh keshwani","photoUrl":"https://lh3.googleusercontent.com/-pJISOj7ZDWg/AAAAAAAAAAI/AAAAAAAAKtc/OZ7PngT-q4k/s64/photo.jpg","userId":"10563007863886487317"}}},"source":["n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print (\"Total Characters: \", n_chars)\n","print (\"Total Vocab: \", n_vocab)\n","\n","n_chars1 = len(raw_text1)\n","n_vocab1 = len(chars1)\n","print (\"Total Characters: \", n_chars1)\n","print (\"Total Vocab: \", n_vocab1)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Total Characters:  142352\n","Total Vocab:  32\n","Total Characters:  2087\n","Total Vocab:  99\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EbFKAh48gio1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9f9fb0ef-e68b-4434-94ac-3c413c484bff","executionInfo":{"status":"ok","timestamp":1564232871902,"user_tz":-330,"elapsed":2651,"user":{"displayName":"utkarsh keshwani","photoUrl":"https://lh3.googleusercontent.com/-pJISOj7ZDWg/AAAAAAAAAAI/AAAAAAAAKtc/OZ7PngT-q4k/s64/photo.jpg","userId":"10563007863886487317"}}},"source":["# prepare the dataset of input to output pairs encoded as integers\n","seq_length = 100\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 1):\n","\tseq_in = raw_text[i:i + seq_length]\n","\t#print(seq_in)\n","\t#seq_in = pad_sequences(seq_in, maxlen=100) #Padded sequence\n","\tseq_out = raw_text[i + seq_length]\n","\tdataX.append([char_to_int[char] for char in seq_in])\n","\tdataY.append(char_to_int[seq_out])\n","n_patterns = len(dataX)\n","print (\"Total Patterns: \", n_patterns)"],"execution_count":50,"outputs":[{"output_type":"stream","text":["Total Patterns:  142252\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eDmwhiv-hA1_","colab_type":"code","colab":{}},"source":["# reshape X to be [samples, time steps, features]\n","X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n","# normalize\n","X = X / float(n_vocab)\n","# one hot encode the output variable\n","y = np_utils.to_categorical(dataY)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ty646WohNUB","colab_type":"code","colab":{}},"source":["# define the LSTM model\n","model = Sequential()\n","model.add(Dropout(0.1))\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n","#model.add(Dropout(0.1))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HfsVuRcHhu3B","colab_type":"code","colab":{}},"source":["# define the checkpoint\n","filepath=\"/content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Mpb-ajSh6wT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2466eac6-7332-4a04-857c-37a95a608832","executionInfo":{"status":"ok","timestamp":1564240161111,"user_tz":-330,"elapsed":7264795,"user":{"displayName":"utkarsh keshwani","photoUrl":"https://lh3.googleusercontent.com/-pJISOj7ZDWg/AAAAAAAAAAI/AAAAAAAAKtc/OZ7PngT-q4k/s64/photo.jpg","userId":"10563007863886487317"}}},"source":["model.fit(X, y, epochs=100, batch_size=256, callbacks=callbacks_list)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","142252/142252 [==============================] - 73s 510us/step - loss: 2.8825\n","\n","Epoch 00001: loss improved from inf to 2.88250, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 2/100\n","142252/142252 [==============================] - 72s 505us/step - loss: 2.7718\n","\n","Epoch 00002: loss improved from 2.88250 to 2.77179, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 3/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 2.7024\n","\n","Epoch 00003: loss improved from 2.77179 to 2.70235, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 4/100\n","142252/142252 [==============================] - 72s 505us/step - loss: 2.6565\n","\n","Epoch 00004: loss improved from 2.70235 to 2.65653, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 5/100\n","142252/142252 [==============================] - 72s 506us/step - loss: 2.6149\n","\n","Epoch 00005: loss improved from 2.65653 to 2.61494, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 6/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 2.5780\n","\n","Epoch 00006: loss improved from 2.61494 to 2.57804, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 7/100\n","142252/142252 [==============================] - 72s 509us/step - loss: 2.5395\n","\n","Epoch 00007: loss improved from 2.57804 to 2.53949, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 8/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 2.5119\n","\n","Epoch 00008: loss improved from 2.53949 to 2.51193, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 9/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 2.4660\n","\n","Epoch 00009: loss improved from 2.51193 to 2.46601, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 10/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 2.4310\n","\n","Epoch 00010: loss improved from 2.46601 to 2.43095, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 11/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 2.3992\n","\n","Epoch 00011: loss improved from 2.43095 to 2.39924, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 12/100\n","142252/142252 [==============================] - 73s 510us/step - loss: 2.3674\n","\n","Epoch 00012: loss improved from 2.39924 to 2.36739, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 13/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 2.3399\n","\n","Epoch 00013: loss improved from 2.36739 to 2.33989, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 14/100\n","142252/142252 [==============================] - 72s 509us/step - loss: 2.3128\n","\n","Epoch 00014: loss improved from 2.33989 to 2.31282, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 15/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 2.2871\n","\n","Epoch 00015: loss improved from 2.31282 to 2.28705, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 16/100\n","142252/142252 [==============================] - 73s 512us/step - loss: 2.2652\n","\n","Epoch 00016: loss improved from 2.28705 to 2.26522, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 17/100\n","142252/142252 [==============================] - 72s 506us/step - loss: 2.2411\n","\n","Epoch 00017: loss improved from 2.26522 to 2.24114, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 18/100\n","142252/142252 [==============================] - 72s 505us/step - loss: 2.2218\n","\n","Epoch 00018: loss improved from 2.24114 to 2.22184, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 19/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 2.1996\n","\n","Epoch 00019: loss improved from 2.22184 to 2.19962, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 20/100\n","142252/142252 [==============================] - 73s 510us/step - loss: 2.1837\n","\n","Epoch 00020: loss improved from 2.19962 to 2.18373, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 21/100\n","142252/142252 [==============================] - 72s 506us/step - loss: 2.1603\n","\n","Epoch 00021: loss improved from 2.18373 to 2.16027, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 22/100\n","142252/142252 [==============================] - 72s 506us/step - loss: 2.1439\n","\n","Epoch 00022: loss improved from 2.16027 to 2.14394, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 23/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 2.1294\n","\n","Epoch 00023: loss improved from 2.14394 to 2.12936, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 24/100\n","142252/142252 [==============================] - 72s 506us/step - loss: 2.1112\n","\n","Epoch 00024: loss improved from 2.12936 to 2.11120, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 25/100\n","142252/142252 [==============================] - 72s 509us/step - loss: 2.0955\n","\n","Epoch 00025: loss improved from 2.11120 to 2.09548, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 26/100\n","142252/142252 [==============================] - 72s 506us/step - loss: 2.0821\n","\n","Epoch 00026: loss improved from 2.09548 to 2.08212, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 27/100\n","142252/142252 [==============================] - 72s 505us/step - loss: 2.0644\n","\n","Epoch 00027: loss improved from 2.08212 to 2.06442, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 28/100\n","142252/142252 [==============================] - 71s 501us/step - loss: 2.0535\n","\n","Epoch 00028: loss improved from 2.06442 to 2.05355, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 29/100\n","142252/142252 [==============================] - 71s 501us/step - loss: 2.0400\n","\n","Epoch 00029: loss improved from 2.05355 to 2.04004, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 30/100\n","142252/142252 [==============================] - 71s 501us/step - loss: 2.0269\n","\n","Epoch 00030: loss improved from 2.04004 to 2.02690, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 31/100\n","142252/142252 [==============================] - 71s 501us/step - loss: 2.0153\n","\n","Epoch 00031: loss improved from 2.02690 to 2.01529, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 32/100\n","142252/142252 [==============================] - 72s 505us/step - loss: 2.0017\n","\n","Epoch 00032: loss improved from 2.01529 to 2.00172, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 33/100\n","142252/142252 [==============================] - 72s 505us/step - loss: 1.9938\n","\n","Epoch 00033: loss improved from 2.00172 to 1.99384, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 34/100\n","142252/142252 [==============================] - 72s 503us/step - loss: 1.9809\n","\n","Epoch 00034: loss improved from 1.99384 to 1.98086, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 35/100\n","142252/142252 [==============================] - 71s 503us/step - loss: 1.9678\n","\n","Epoch 00035: loss improved from 1.98086 to 1.96776, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 36/100\n","142252/142252 [==============================] - 71s 503us/step - loss: 1.9606\n","\n","Epoch 00036: loss improved from 1.96776 to 1.96060, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 37/100\n","142252/142252 [==============================] - 72s 504us/step - loss: 1.9460\n","\n","Epoch 00037: loss improved from 1.96060 to 1.94603, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 38/100\n","142252/142252 [==============================] - 72s 506us/step - loss: 1.9373\n","\n","Epoch 00038: loss improved from 1.94603 to 1.93728, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 39/100\n","142252/142252 [==============================] - 72s 504us/step - loss: 1.9297\n","\n","Epoch 00039: loss improved from 1.93728 to 1.92974, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 40/100\n","142252/142252 [==============================] - 72s 504us/step - loss: 1.9148\n","\n","Epoch 00040: loss improved from 1.92974 to 1.91482, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 41/100\n","142252/142252 [==============================] - 72s 503us/step - loss: 1.9094\n","\n","Epoch 00041: loss improved from 1.91482 to 1.90938, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 42/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.9017\n","\n","Epoch 00042: loss improved from 1.90938 to 1.90170, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 43/100\n","142252/142252 [==============================] - 72s 506us/step - loss: 1.8913\n","\n","Epoch 00043: loss improved from 1.90170 to 1.89134, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 44/100\n","142252/142252 [==============================] - 72s 506us/step - loss: 1.8824\n","\n","Epoch 00044: loss improved from 1.89134 to 1.88236, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 45/100\n","142252/142252 [==============================] - 72s 506us/step - loss: 1.8724\n","\n","Epoch 00045: loss improved from 1.88236 to 1.87240, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 46/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.8680\n","\n","Epoch 00046: loss improved from 1.87240 to 1.86798, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 47/100\n","142252/142252 [==============================] - 72s 510us/step - loss: 1.8581\n","\n","Epoch 00047: loss improved from 1.86798 to 1.85812, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 48/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.8477\n","\n","Epoch 00048: loss improved from 1.85812 to 1.84775, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 49/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.8409\n","\n","Epoch 00049: loss improved from 1.84775 to 1.84089, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 50/100\n","142252/142252 [==============================] - 73s 513us/step - loss: 1.8339\n","\n","Epoch 00050: loss improved from 1.84089 to 1.83392, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 51/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 1.8263\n","\n","Epoch 00051: loss improved from 1.83392 to 1.82632, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 52/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.8174\n","\n","Epoch 00052: loss improved from 1.82632 to 1.81744, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 53/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.8147\n","\n","Epoch 00053: loss improved from 1.81744 to 1.81466, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 54/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 1.8038\n","\n","Epoch 00054: loss improved from 1.81466 to 1.80378, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 55/100\n","142252/142252 [==============================] - 73s 511us/step - loss: 1.8004\n","\n","Epoch 00055: loss improved from 1.80378 to 1.80037, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 56/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 1.7939\n","\n","Epoch 00056: loss improved from 1.80037 to 1.79386, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 57/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.7866\n","\n","Epoch 00057: loss improved from 1.79386 to 1.78661, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 58/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.7883\n","\n","Epoch 00058: loss did not improve from 1.78661\n","Epoch 59/100\n","142252/142252 [==============================] - 73s 512us/step - loss: 1.7809\n","\n","Epoch 00059: loss improved from 1.78661 to 1.78092, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 60/100\n","142252/142252 [==============================] - 73s 510us/step - loss: 1.7721\n","\n","Epoch 00060: loss improved from 1.78092 to 1.77212, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 61/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.7670\n","\n","Epoch 00061: loss improved from 1.77212 to 1.76703, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 62/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 1.7612\n","\n","Epoch 00062: loss improved from 1.76703 to 1.76123, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 63/100\n","142252/142252 [==============================] - 73s 510us/step - loss: 1.7511\n","\n","Epoch 00063: loss improved from 1.76123 to 1.75112, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 64/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.7442\n","\n","Epoch 00064: loss improved from 1.75112 to 1.74419, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 65/100\n","142252/142252 [==============================] - 72s 506us/step - loss: 1.7425\n","\n","Epoch 00065: loss improved from 1.74419 to 1.74252, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 66/100\n","142252/142252 [==============================] - 72s 509us/step - loss: 1.7374\n","\n","Epoch 00066: loss improved from 1.74252 to 1.73737, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 67/100\n","142252/142252 [==============================] - 73s 511us/step - loss: 1.7309\n","\n","Epoch 00067: loss improved from 1.73737 to 1.73087, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 68/100\n","142252/142252 [==============================] - 73s 512us/step - loss: 1.7234\n","\n","Epoch 00068: loss improved from 1.73087 to 1.72340, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 69/100\n","142252/142252 [==============================] - 73s 512us/step - loss: 1.7221\n","\n","Epoch 00069: loss improved from 1.72340 to 1.72210, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 70/100\n","142252/142252 [==============================] - 73s 513us/step - loss: 1.7169\n","\n","Epoch 00070: loss improved from 1.72210 to 1.71686, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 71/100\n","142252/142252 [==============================] - 73s 512us/step - loss: 1.7127\n","\n","Epoch 00071: loss improved from 1.71686 to 1.71274, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 72/100\n","142252/142252 [==============================] - 73s 516us/step - loss: 1.7048\n","\n","Epoch 00072: loss improved from 1.71274 to 1.70476, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 73/100\n","142252/142252 [==============================] - 73s 511us/step - loss: 1.7003\n","\n","Epoch 00073: loss improved from 1.70476 to 1.70028, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 74/100\n","142252/142252 [==============================] - 73s 510us/step - loss: 1.6952\n","\n","Epoch 00074: loss improved from 1.70028 to 1.69522, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 75/100\n","142252/142252 [==============================] - 73s 511us/step - loss: 1.6940\n","\n","Epoch 00075: loss improved from 1.69522 to 1.69403, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 76/100\n","142252/142252 [==============================] - 73s 513us/step - loss: 1.6857\n","\n","Epoch 00076: loss improved from 1.69403 to 1.68569, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 77/100\n","142252/142252 [==============================] - 73s 510us/step - loss: 1.6796\n","\n","Epoch 00077: loss improved from 1.68569 to 1.67960, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 78/100\n","142252/142252 [==============================] - 72s 509us/step - loss: 1.6830\n","\n","Epoch 00078: loss did not improve from 1.67960\n","Epoch 79/100\n","142252/142252 [==============================] - 72s 509us/step - loss: 1.6732\n","\n","Epoch 00079: loss improved from 1.67960 to 1.67325, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 80/100\n","142252/142252 [==============================] - 73s 516us/step - loss: 1.6725\n","\n","Epoch 00080: loss improved from 1.67325 to 1.67254, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 81/100\n","142252/142252 [==============================] - 73s 512us/step - loss: 1.6625\n","\n","Epoch 00081: loss improved from 1.67254 to 1.66251, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 82/100\n","142252/142252 [==============================] - 73s 513us/step - loss: 1.6602\n","\n","Epoch 00082: loss improved from 1.66251 to 1.66022, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 83/100\n","142252/142252 [==============================] - 73s 512us/step - loss: 1.6603\n","\n","Epoch 00083: loss did not improve from 1.66022\n","Epoch 84/100\n","142252/142252 [==============================] - 73s 515us/step - loss: 1.6542\n","\n","Epoch 00084: loss improved from 1.66022 to 1.65423, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 85/100\n","142252/142252 [==============================] - 73s 514us/step - loss: 1.6554\n","\n","Epoch 00085: loss did not improve from 1.65423\n","Epoch 86/100\n","142252/142252 [==============================] - 72s 509us/step - loss: 1.6452\n","\n","Epoch 00086: loss improved from 1.65423 to 1.64515, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 87/100\n","142252/142252 [==============================] - 73s 510us/step - loss: 1.6432\n","\n","Epoch 00087: loss improved from 1.64515 to 1.64320, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 88/100\n","142252/142252 [==============================] - 73s 511us/step - loss: 1.6435\n","\n","Epoch 00088: loss did not improve from 1.64320\n","Epoch 89/100\n","142252/142252 [==============================] - 73s 513us/step - loss: 1.6338\n","\n","Epoch 00089: loss improved from 1.64320 to 1.63381, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 90/100\n","142252/142252 [==============================] - 73s 511us/step - loss: 1.6315\n","\n","Epoch 00090: loss improved from 1.63381 to 1.63148, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 91/100\n","142252/142252 [==============================] - 72s 509us/step - loss: 1.6267\n","\n","Epoch 00091: loss improved from 1.63148 to 1.62665, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 92/100\n","142252/142252 [==============================] - 72s 509us/step - loss: 1.6244\n","\n","Epoch 00092: loss improved from 1.62665 to 1.62438, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 93/100\n","142252/142252 [==============================] - 73s 513us/step - loss: 1.6199\n","\n","Epoch 00093: loss improved from 1.62438 to 1.61993, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 94/100\n","142252/142252 [==============================] - 72s 509us/step - loss: 1.6188\n","\n","Epoch 00094: loss improved from 1.61993 to 1.61881, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 95/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.6143\n","\n","Epoch 00095: loss improved from 1.61881 to 1.61432, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 96/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.6106\n","\n","Epoch 00096: loss improved from 1.61432 to 1.61057, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 97/100\n","142252/142252 [==============================] - 73s 511us/step - loss: 1.6095\n","\n","Epoch 00097: loss improved from 1.61057 to 1.60948, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 98/100\n","142252/142252 [==============================] - 72s 509us/step - loss: 1.6063\n","\n","Epoch 00098: loss improved from 1.60948 to 1.60634, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 99/100\n","142252/142252 [==============================] - 72s 507us/step - loss: 1.6028\n","\n","Epoch 00099: loss improved from 1.60634 to 1.60281, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n","Epoch 100/100\n","142252/142252 [==============================] - 72s 508us/step - loss: 1.6013\n","\n","Epoch 00100: loss improved from 1.60281 to 1.60128, saving model to /content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fc050841f98>"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"yjimPMopkjxt","colab_type":"code","colab":{}},"source":["# load the network weights\n","filename = \"/content/gdrive/My Drive/Colab Notebooks/EIP 3/assign2.hdf5\"\n","model.load_weights(filename)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RPBnBwzLksWP","colab_type":"code","colab":{}},"source":["int_to_char = dict((i, c) for i, c in enumerate(chars))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sap0e54Qkvd_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":188},"outputId":"97ec3b5d-3d72-4524-f86b-cfb2ba311d8d","executionInfo":{"status":"ok","timestamp":1564247453629,"user_tz":-330,"elapsed":24986,"user":{"displayName":"utkarsh keshwani","photoUrl":"https://lh3.googleusercontent.com/-pJISOj7ZDWg/AAAAAAAAAAI/AAAAAAAAKtc/OZ7PngT-q4k/s64/photo.jpg","userId":"10563007863886487317"}}},"source":["# pick a random seed\n","start = numpy.random.randint(0, len(dataX)-1)\n","pattern = dataX[start]\n","print (\"Seed:\")\n","print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","# generate characters\n","for i in range(500):\n","\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n","\tx = x / float(n_vocab)\n","\tprediction = model.predict(x, verbose=0)\n","\tindex = numpy.argmax(prediction)\n","\tresult = int_to_char[index]\n","\tseq_in = [int_to_char[value] for value in pattern]\n","\tsys.stdout.write(result)\n","\tpattern.append(index)\n","\tpattern = pattern[1:len(pattern)]\n","print (\"\\nDone.\")"],"execution_count":57,"outputs":[{"output_type":"stream","text":["Seed:\n","\" and turns out his toes.\n","\n","              later editions continued as follows\n","    when the sands are al \"\n","l to yeel and the selter wiat soe eonled at eoulousoe io she hrr thmerken.\n","\n","  `how sheay oo toe se the seid te the waic to herself athe wuon of the herter and the whmnght in  oo tie fraligot and the wei  `o ce rateerer the sioule lild to the and the wai  `o ce ruoeree in serty.nf herself in soment bnd the whmught tt herself and the wem onhh the led thait hear she wooe tie whou sie heard i could to  here whe soece uf the hoass rot aelon her hnn she her and the whing to beant  andce what she wioug\n","Done.\n"],"name":"stdout"}]}]}